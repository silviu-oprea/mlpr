{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer layer\n",
    "\n",
    "A transformer layer takes a sequence of $T$ word vectors and returns another sequence of $T$ vectors. Intuitively, it computes the contextual re-representation of each word vector - where the context are all the other word vectors in the sequence.\n",
    "\n",
    "Given $T$ input word vectors $x_{1:T}$, it computes $T$ weighted averages $z_{1:T}$ of the inputs. Each $z_T$ is a representation of $x_{1:T}$ in the context of all other $x_{\\ne t}$. It does this by fitting $T$ attention functions $a_{1:T}$ (before we only had one).\n",
    "\n",
    "<img src=\"img/5_neural_networks_transformer.drawio.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the idea, but more is going on. Here is a more detailed depiction of an encoder-decoder transformer. The image is taken from Jay Alammar's awesome blog post, [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).\n",
    "In the image: the thicker dotted lines are skip-connections.\n",
    "\n",
    "<img src=\"img/8_transformers/transformer_architecture.png\" width=\"650\">\n",
    "\n",
    "Here is another awesome visualisation.\n",
    "\n",
    "<img src=\"img/8_transformers/transformer_decoding_2.gif\" width=\"550\">\n",
    "\n",
    "Let's look at each component. In what follows:\n",
    "* We use row vectors.\n",
    "* We assume $T$ input word vectors $x$ stacked into a matrix $X \\in \\mathbb{R}^{\\text{doc len} \\times \\text{embed dim}}$. For instance, the first image under the Self-Attention section below illustrates $X \\in \\mathbb{R}^{2 \\times 4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Limitations of Word (space) tokenization:\n",
    "* Large vocabularies. Even more likely to occur for languages with a righ inflectional morphology, where different suffixes attached to word roots express different genders, numbers, or cases (accusative, genitive, dative).\n",
    "* We will impose a limit on vocabualary size. But then we'll have out-of-vocabulary tokens\n",
    "* Going back to inflectional morphology: loss of meaning between very similar words. E.g. \"phone\" and \"phones\" will be two separate vocabulary entries.\n",
    "\n",
    "Limitations of character-based tokenization:\n",
    "* Very long token sequences, each token being only one character long;\n",
    "* When braking words into individual characters, we miss the meaning resulted from glueing characters together.\n",
    "\n",
    "Subword tokenisation aims to increase coverage of dictionaries. Combining subword tokens seen during training, we can represent words unseen during training.\n",
    "Here are popular subword tokenization algorithms.\n",
    "\n",
    "Byte-pair encoding:\n",
    "* Keep frequent words in their original form.\n",
    "* Break down infrequent words.\n",
    "\n",
    "Wordpiece vs BPE:\n",
    "* BPE adds most frequent tokens to the vocabulary; Wordpiece adds those that maximise the likelihood of the training data. Merge t1 and t2 if p(t1t2)/[p(t1)p(t2)] is largest among all t1 and t2 pairs.\n",
    "\n",
    "Wordpiece and BPE assume input string is tokenised, e.g. space tokenised. \n",
    "Sentencepiece does not. Ja and zh do not have explicit space.\n",
    "\n",
    "After tokenization, we can add special tokens, such as the start of sequence token, often denoted as `<s>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input encoding\n",
    "\n",
    "The output of tokenization is a list of integers of length (doc_len), each being a vocabulary index, e.g. `[1, 8, 3, 2]`. Given two or more lists, e.g. `[1, 8, 3, 2]` and `[1, 10, 2]`, we can batch them into a tensor of dimension `(batch_size, max batch doc_len)` `[[1, 8, 3, 2], [1, 10, 2, 0]]`. Note we have padded the shorter list with the index of the padding token in the vocabulary, i.e. 0 in our example. This is the input tensor.\n",
    "\n",
    "Next, we have an embedding layer; this is parametrised by an embedding matrix $E$ of dimension `(vocab_len, embed_dim)`.\n",
    "Row $e_i$ of this matrix, of dimension `embed_dim`, contains the vector representation, i.e. embedding, of word at index $i$ in the vocabulary.\n",
    "We replace each vocabulary index in the input tensor with the corresponding embedding: $[[e_1, e_8, e_3, e_2], [e_1, e_{10}, e_2, e_0]]$.\n",
    "This replacement can be simulated by: computing the one-hot (row) vector representation of each position; and multiplying that vector with the embedding matrix.\n",
    "\n",
    "Next, we have a positional encoding layer; this is parametrised by a matrix $P$ of dimension `(max allowed doc_len, embed_dim)`. We create the position tensor $[[p_0, p_1, p_2, p_3], [p_0, p_1, p_2, p_3]]$, i.e. simply take as many position vectors as we have tokens.\n",
    "\n",
    "Finally, we sum the embedding tensor and the input tensor. The result is the input to the first transformer layer (encoder or decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "## Self-attention\n",
    "\n",
    "Here are the steps performed **for each** input word vector $x_t$.\n",
    "\n",
    "First, compute three vectors: a query $q_t$, a key $k_t$, and a value $v_t$. To create the query, multiply $x_t$ by a query matrix that we learn (i.e. params in this matrix adapted during training): $q_t = x_t W^Q$. Proceed analogously for the key and value vectors. Note: we can stack all $T$ querries into a matrix $Q = X  W^Q$; In our example, $Q \\in \\mathbb{R}^{2 \\times 3}$, $X \\in \\mathbb{R}^{2 \\times 4}$, and $W^Q \\in \\mathbb{R}^{4 \\times 3}$.\n",
    "\n",
    "<img src=\"img/8_transformers/self-attention-matrix-calculation.png\" width=\"250\">\n",
    "\n",
    "Second, compute $T$ scores $\\sigma_{1:T}$, where $\\sigma_{i} = q_t k_i$ (dot product). Intuitively, $\\sigma_{i}$ measures how much focus to place on word $i$ when encoding current word $t$. For instance, in \"I ate a pizza, it was delicious\", when encoding \"it\", the score corresponding to its referrent \"pizza\" should be largest, which the score corresponding to \"I\" should be small.\n",
    "\n",
    "Third, ivide each $\\sigma_i$ by 8, i.e. square root of 64, i.e. dimension of the key vectors used in the \"Attention is all you Need\" paper. Then compute softmax over $\\sigma_{1:T}$. Dividing by 8 leads to a more even distribution of the probability mass, thus less mass values close to 0, thus more stable gradients.\n",
    "\n",
    "Finally, multiply each value vector with the corresponding score and sum the results to compute a representation $z_t$ of the current word: $z_t = \\sum_{i=1}^T \\sigma_i v_i$.\n",
    "\n",
    "<img src=\"img/8_transformers/self-attention-output.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Self-attention:\n",
    "* Given a token, call it the current token, compute an affinity score between the current token and every token in the sequence, resulting in T affinity scores.\n",
    "* To accomplish this, every token emits three vectors: a query, a key, and a value:\n",
    "  * query of a token: intuitively expresses what that token is looking for when considring other tokens. \"It\" looking for a referrant.\n",
    "  * key: information content that other queries can match against.\n",
    "  * value: what the token communicates about itself to other tokens.\n",
    "* The affinity score between the current token and another token is the dot product between query(current token) and key(other token).\n",
    "* When re-representing current token, we compute affinity between the current token and all previous tokens, including current token. We then represent current token as the weighted average of the values of all these tokens, where the weights are the affinity scores.\n",
    "* Summary: key (what I contain), query (what I am looking for), value (what I will communicate about myself).\n",
    "\n",
    "Analogy:\n",
    "* Directed graph. Every node has some value vector. This expresses the information content it communicates to nodes that it points to.\n",
    "* Given a node, compute a weighted average of the nodes that point to this node. We want the weights to be data dependent.\n",
    "\n",
    "When re-representing a specific token:\n",
    "* Query: A representation of that specific token; representation that we score against all other tokens.\n",
    "* Keys: For each token, a representation that we match the query to.\n",
    "* (We match the query to each key to compute a relevance score; dot product and softmax).\n",
    "* Values: The representations we average (weighted by the scores above) to compute a representation of the current word.\n",
    "\n",
    "Analogy: searching in a filing cabinet.\n",
    "* Query: A sticky note summarising the information we are looking for.\n",
    "* Keys: For each folder, the key is the label summarising the information in that folder.\n",
    "* (We match the query to each key to compute a relevance score for each folder)\n",
    "* Values: For each folder, the information unit in the folder. We merge all information units, each being assigned a \"trust\" score computed before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Assume we are given a tensor as the one below, of size (batch size, doc_len, emb_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      2\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      3\u001b[0m      [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m      [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      7\u001b[0m      [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      8\u001b[0m      [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      9\u001b[0m ])\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [[1, 3],\n",
    "     [2, 1],\n",
    "     [0, 1]],\n",
    "\n",
    "    [[0, 1],\n",
    "     [5, 4],\n",
    "     [0, 0]]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to produce another where the representation of word $t \\in \\{1, \\ldots, \\text{doc\\_len}\\}$ is the sum representations of words $1, \\ldots, t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      2\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      3\u001b[0m      [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      4\u001b[0m      [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      7\u001b[0m      [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m      8\u001b[0m      [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m]]\n\u001b[1;32m      9\u001b[0m ])\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([\n",
    "    [[1, 3],\n",
    "     [3, 4],\n",
    "     [3, 5]],\n",
    "\n",
    "    [[0, 1],\n",
    "     [5, 5],\n",
    "     [5, 5]]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the inefficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_bow = torch.zeros_like(x)\n",
    "batch_size, doc_len, emb_dim = x.size()\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(doc_len):\n",
    "        x_bow[b, t] = x[b, :t + 1].sum(dim=0)\n",
    "x_bow.allclose(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[1., 3.],\n",
      "         [2., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [5., 4.],\n",
      "         [0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 3.],\n",
       "         [3., 4.],\n",
       "         [3., 5.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [5., 5.],\n",
       "         [5., 5.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3,3))\n",
    "print(mask)\n",
    "print(x)\n",
    "mask @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are multiplying mask of size (3, 3) with x of size (2, 3, 2). Pytorch will know to multiply mask with each (3, 2) element in x.\n",
    "\n",
    "Now let's do averaging instead of summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 3.0000],\n",
       "         [1.5000, 2.0000],\n",
       "         [1.0000, 1.6667]],\n",
       "\n",
       "        [[0.0000, 1.0000],\n",
       "         [2.5000, 2.5000],\n",
       "         [1.6667, 1.6667]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3))\n",
    "mask[mask == 0] = float(\"-inf\")\n",
    "mask = torch.softmax(mask, dim=1)\n",
    "print(mask)\n",
    "\n",
    "mask @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, when computing the representation of the current token, all previous tokens are given the same weight.\n",
    "This is not great. Consider \"I had a pizza, it was very tasty\". When representing \"it\", (the representation of) \"pizza\" should have a higher weight that that of \"had\", for instance.\n",
    "\n",
    "For a character-level model, the representation of a e.g. vowel should depend on specific previous consonants.\n",
    "\n",
    "We want such weights to be data dependent. This is the problem that self-attention solves.\n",
    "\n",
    "Self-attention:\n",
    "* Given a token, call it the current token, compute an affinity score between the current token and every token in the sequence, resulting in T affinity scores.\n",
    "* To accomplish this, every token emits three vectors: a query, a key, and a value:\n",
    "  * query of a token: intuitively expresses what that token is looking for when considring other tokens. \"It\" looking for a referrant.\n",
    "  * key: information content that other queries can match against.\n",
    "  * value: what the token communicates about itself to other tokens.\n",
    "* The affinity score between the current token and another token is the dot product between query(current token) and key(other token).\n",
    "* When re-representing current token, we compute affinity between the current token and all previous tokens, including current token. We then represent current token as the weighted average of the values of all these tokens, where the weights are the affinity scores.\n",
    "* Summary: key (what I contain), query (what I am looking for), value (what I will communicate about myself).\n",
    "\n",
    "Analogy:\n",
    "* Directed graph. Every node has some value vector. This expresses the information content it communicates to nodes that it points to.\n",
    "* Given a node, compute a weighted average of the nodes that point to this node. We want the weights to be data dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 3.],\n",
       "          [2., 1.],\n",
       "          [0., 1.]],\n",
       " \n",
       "         [[0., 1.],\n",
       "          [5., 4.],\n",
       "          [0., 0.]]]),\n",
       " torch.Size([2, 3, 2]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size, doc_len, emb_dim = x.size()\n",
    "x, x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 16])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B, T, C = x.size()\n",
    "head_size = 16 # the dimension of the key, query, and value vectors\n",
    "\n",
    "key_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
    "query_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
    "value_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "query = query_layer(x) # (B, T, C)\n",
    "key = key_layer(x)     # (B, T, C)\n",
    "value = value_layer(x) # (B, T, C)\n",
    "\n",
    "# (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "wei = query @ key.transpose(2, 1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = torch.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ value\n",
    "\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other self attention layers\n",
    "\n",
    "* There is no notion of space. This is why we need positional encoding. Meaning of a token might differ with position. E.g. verb particles in German added to the end of the sentence Self-attention vs convolution: notion of space exists in convolution.\n",
    "* Above we are implementing a _decoder_ / _masked_ self-attention block; when representing the current token, we do NOT consider future tokens. By contrast, in _encoder_ block, this restriction is removed; implemented by removing the masking, i.e. removing `wei = wei.masked_fill(tril == 0, float(\"-inf\"))`. The decoder is appropriate for language modelling; encoder for e.g. classification tasks, such as sentiment analysis.\n",
    "* In self-attention, keys, querries, and values, are computed from the same source. In _cross-attention_, also referred to as _encoder-decoder attention_ when using an encoder-decoder architecture, the querries are computed from current source, but keys and values from another source, from which we would like to pull information; for instance, from the (output of the) encoder blocks, that represent some context we'd like to condition on.\n",
    "* _Scaled_ attention divides `wei` by `sqrt(head_size)`. Assume querries and keys are unit Gaussians, i.e. follow a Gaussian distribution with unit variance. If we do not divide, the variance of wei will be multiplied by `head_size`. If we divide, it stays unit variance. Stated differently, the values in `wei` will simply be larger. As a result, the softmax will sharper, getting further away from a uniform, sharping towards the max, converging to a one-hot vector.\n",
    "* _Multi-head_ attention: apply multiple attentions in parallel (using different query, key, and value matrices) and concatenate their results. Having different initialisations, they capture different interactions. Different local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8659)\n",
      "tensor(0.7215)\n",
      "tensor(8.2634)\n",
      "tensor(0.5165)\n"
     ]
    }
   ],
   "source": [
    "Q = torch.randn(B, T, head_size)\n",
    "K = torch.randn(B, T, head_size)\n",
    "wei_1 = Q @ K.transpose(1, 2)\n",
    "wei_2 = Q @ K.transpose(1, 2) / head_size**0.5\n",
    "\n",
    "print(Q.var())\n",
    "print(K.var())\n",
    "print(wei_1.var())\n",
    "print(wei_2.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple heads\n",
    "\n",
    "We can stack all $z_{1:T}$ into a matrix $Z$ computed as $\\text{softmax}\\left( \\frac{QK^{Tr}}{\\sqrt{64}} \\right) V $, where softmax is applied row-wise.<br/>\n",
    "<img src=\"img/8_transformers/self-attention-matrix-calculation-2.png\" width=\"400\">\n",
    "\n",
    "We can perform the same operations $H$ times, using $H$ query, key, and value matrices, resulting in different outputs $z_{t,h}$. Each corresponds to what is referred to as a *head*. Intuitively, we want each $z_{t,h}$ to capture different interactions of word $t$ with its contextual words. \n",
    "In this purpose, we initialise different query, key, and value matrices **to different values** across different heads. Eventually we can concatenate all $z_{t, h}$ into a large vector of dimension $H|z_t|$; and map it to a vector $z_t$ using a linear layer, i.e. multiplying by a param matrix $W^0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Decoder\n",
    "\n",
    "Different from the encoder, the decoder:\n",
    "* replaces self-attention with masked self-attention; and\n",
    "* has an extra encoder-decoder layer, also referred to as a cross-attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Masked self-attention\n",
    "\n",
    "Say we are at decoding step $t$. When computing the scores $\\sigma_{1:S}$ ($S$ denotes the maximum number of decoding steps), make sure $\\sigma_{>t} = 0$. This is achieved by setting the scores on positions $>t$ to `-inf` before applying the softmax.\n",
    "\n",
    "Intuitively, a self-attention layer inputs a sequence of word vectors, i.e. embeddings; for each embedding, call it the current embedding, the layer computes its contextual re-representation, where the context are all the other embeddings in the sequence.\n",
    "In a masked self-attention layer, the context are all embeddings that occur before the current embedding in the sequence (and the current embedding itself).\n",
    "\n",
    "<img src=\"img/8_transformers/masked-self-attention-2.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder attention\n",
    "\n",
    "The output of the top encoder is transformed into key and value vectors, respectively; assume they are stacked into key and value matrices $K_{\\text{enc-dec}}$ and $V_{\\text{enc-dec}}$, respectively.\n",
    "\n",
    "These are used in the encoder-decoder attetion layer in the decoder. This works like self-attention, except:\n",
    "* The key and value matrices are $K_{\\text{enc-dec}}$ and $V_{\\text{enc-dec}}$; while the query matrix is computed from the output of previous decoder blocks.\n",
    "* Say we are at decoding step $t$. When computing the scores $\\sigma_{1:T}$ ($T$ now denotes the maximum number of decoding steps), make sure $\\sigma_{>t} = 0$. This is achieved by setting the scores on positions $>t$ to `-inf` before applying the softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "At each decoding step, the top decoder produces a vector; this is the final contextual re-representation of the input token at that step (where the context are the current and previous tokens).\n",
    "\n",
    "Next, using a linear layer, we map this final vector to a vector of probabilities over the vocabulary.\n",
    "The linear layer can be parametrised by the original embedding matrix. As such, position $i$ in the vector of probabilities is the dot product between the final vector and embedding $e_i$.\n",
    "\n",
    "This vector of probabilities is then sampled to produce an output token at this step.\n",
    "\n",
    "<img src=\"img/8_transformers/gpt2-output.png\" width=\"700\">\n",
    "\n",
    "Decoding continues until:\n",
    "* an end-of-sequence token is produced; or\n",
    "* the maximum number of tokens was generated, e.g. 1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful tips for optimising deep neural networks\n",
    "\n",
    "## Skip-connections\n",
    "\n",
    "Also known as residual connections. The idea originates in the paper \"Deep Residual Learning for Image Recognition\" by [He et al. (2015)](https://arxiv.org/pdf/1512.03385).\n",
    "\n",
    "There is a residual pathway. We branch off, perform computation (e.g. self-attention), and integrate the result in the residual pathway via addition.\n",
    "\n",
    "Addition distributes gradients equally to both branches during backprop. The gradients this way flow from loss to the input via the residual path. Helps optimisation: preventing vanishing gradients; and importantly, every param will be optimised in terms of the supervision signal directly.\n",
    "\n",
    "## Layer Normalisation\n",
    "\n",
    "Batch normalisation: across the batch dimension, every individual neuron output has unit Gaussian distribution, i.e. 0 mean, unit standard deviation.\n",
    "\n",
    "Pre-norm formulation: layer normalisation applied before transformations (self-attention and linear). As opposed to after, as in the original transformer paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "## Language modelling\n",
    "\n",
    "Training data:\n",
    "* Start with a corpus of text, e.g. \"robots must protect humans\" - in our example, this is the entire corpus.\n",
    "* Generate training examples: (input=robots, target=must), (input=robots must, target=protect), (input=robots must protect, target=humans), (input=robots must protect humans, target=\\<\\/s\\>).\n",
    "\n",
    "In a maximum likelihood approach to parameter estimation (i.e. to training):\n",
    "* At each training step, provide the input to the model; compare its output to the target to compute the loss; fine-tune model parameters to minimise the loss.\n",
    "* In more detail, the target one-hot encoded; a trained model output is a vector of probabilities over the vocabulary.\n",
    "\n",
    "The loss could be the sum of cross-entropies, one for each position.\n",
    "\n",
    "<img src=\"img/8_transformers/output_target_probability_distributions.png\" width=\"400\">\n",
    "<img src=\"img/8_transformers/output_trained_model_probability_distributions.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:\n",
    "* BERT: stack of encoders\n",
    "* GPT-2: stack of decoders, bype-pair encoding\n",
    "* GPT-3: trained on 300B tokens, 96 transformer layers 1.8B params each, 2048 context window (maximum input length), 175B params.\n",
    "\n",
    "A stack of encoders can be used for masked language modelling.\n",
    "\n",
    "A decoder in a stack of decoders does not have the encoder-decoder attention layer - obviously.\n",
    "A stack of decoders can be used for autoregressive language modelling: output one token at a time; and token generated at step $t$ becomes an input for generation at step $t+1$.\n",
    "\n",
    "<img src=\"img/8_transformers/gpt-2-autoregression-2.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "salut ce mai faci [toEnglish] hello how are you\n",
    "\n",
    "Generate code based on the input prompt\n",
    "[example] an input that says \"search\" [toCode] Class App extends Reach Component ... }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "Terminology:\n",
    "* There is an agent interacting with the environment by taking an action. It uses a policy (e.g. LLM) to map a state to an action.\n",
    "* In response, the environment returns a state (state of the word as a result of the action) and a reward (to maximise).\n",
    "\n",
    "1. Pre-train language model on a large corpus of text. Optionally, include text generated by humans; for instance, given a popular question, employ humans to write a high quality answer.\n",
    "2. Start with a dataset of prompts, such as questions asked by ChatGPT users. Provide each prompt to one or more language models to generate multiple answers. Employ human annotators to rank the answers, providing each answer a score. The result is a dataset of pairs (prompt, answer, score).\n",
    "3. Train a reward model on the dataset above that, given a prompt and and answer, predicts the score.\n",
    "\n",
    "Pipeline:\n",
    "* Terminology:\n",
    "  * policy: an LLM;\n",
    "  * state: model inputs, i.e. natural language;\n",
    "  * action: model output, i.e. natural language.\n",
    "* policy(state, e.g. question) = action e.g. answer.\n",
    "* reward model(action) = reward.\n",
    "* The policy can overfit the data generated by the reward model, i.e. its params can be adapted in such a way to always maximise the reward, but the coherence of the generated text might suffer. To prevent this, we impose a maximum distance (KL divergence) between the final vocabulary distributions generated by the policy and that of the initial language model (before RLHF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
