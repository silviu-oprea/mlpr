{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "\n",
    "Maximum likelihood (ML):\n",
    "- is an algorithm;\n",
    "- for fitting, i.e. estimating the parameters $\\underline\\theta$ of;\n",
    "- a probability distribution/measure, i.e. a parametric function $f_{\\underline\\theta}$ from a predetermined family;\n",
    "- given $N$ <u>presumed</u> samples from that distribution.\n",
    "\n",
    "Steps:\n",
    "1. Formulate the likelihood function $\\mathcal{L}(\\underline\\theta) = \\prod_{n=1}^N p(y^{(n)} | \\underline x^{(n)}; f_{\\underline\\theta})$. Treat this as a \"loss\" or \"cost\" function.\n",
    "2. Find parameters $\\underline \\theta$ that maximise $\\mathcal{L}(\\underline \\theta)$.\n",
    "\n",
    "We usually solve the equivalent problem of minimising $- \\log \\mathcal{L}(\\underline \\theta)$, easier for easier computation.\n",
    "\n",
    "Based on the distribution family chosen, we get different losses. Here are some common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square loss\n",
    "Assume\n",
    "$$\n",
    "p\\left(y^{(n)} | x^{(x)}; f_{\\underline \\theta}\\right) = \\mathcal{N} \\left( y^{(n)}; \\mu = f\\left( x^{(n)} \\right), \\sigma_y^2 \\right),\n",
    "$$\n",
    "for some fixed $\\sigma_y^2$, which (assume) can be estimated with e.g. a neural network $f_{\\underline\\theta}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\mathcal{L}(\\underline \\theta) = c_1 \\sum_n \\left( y^{(n)} - f_{\\underline \\theta}\\left( \\underline x^{(n)} \\right) \\right)^2 + c_2,\n",
    "$$\n",
    "for some (irrelevant for optimisation) constants $c_1$ and $c_2$.\n",
    "\n",
    "This is the regular sum of squares loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy loss\n",
    "\n",
    "Think about binary classification tasks. That is, assume\n",
    "$$\n",
    "p\\left( y^{(n)} | x^{(n)}; f_{\\underline \\theta} \\right) =\n",
    "\\begin{cases}\n",
    "    p^{(n)}, & \\text{if } y^{(n)} = 1\\\\\n",
    "    1 - p^{(n)}, & \\text{if } y^{(n)} = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "where $p^{(n)} \\stackrel{\\text{e.g.}}{=} \\sigma\\left( f_{\\underline\\theta}\\left( x^{(n)} \\right) \\right)$, which (assume) can be estimated with a e.g. neural network $f_{\\underline\\theta}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\mathcal{L}(\\underline\\theta) = \\sum_{n:y^{(n)} = 1} \\log p^{(n)} - \\sum_{n: y^{(n)} = 0} \\log \\left( 1 - p^{(n)} \\right) = \\sum_n \\left( p^{(n)} \\right)^{y^{(n)}} \\left( 1 - p^{(n)} \\right)^{\\left( 1 - y^{(n)} \\right)}.\n",
    "$$\n",
    "\n",
    "Now say we have more than two classes. Let $p( y^{(n)} | x^{(n)} ) = p^{(n)}$. Given training set $T=\\{(x^{(n)}, y^{(n)})\\}$, $\\mathcal{L}(\\underline\\theta) = \\sum_n p^{(n)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5500)\n",
      "tensor(-0.5500)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# E.g. for a NN with 3 units in the final layer, to which we applied softmax.\n",
    "probs = torch.tensor([\n",
    "    [0.1, 0.4, 0.5],\n",
    "    [0.7, 0.1, 0.2]\n",
    "])\n",
    "labels = torch.tensor([1, 0])\n",
    "\n",
    "nll_loss = -probs.gather(dim=1, index=labels.unsqueeze(1)).mean()\n",
    "print(nll_loss)\n",
    "\n",
    "# Alternatively, without using \"gather\"\n",
    "nll_loss = torch.zeros_like(labels, dtype=torch.float)\n",
    "for row in range(probs.size(0)):\n",
    "    nll_loss[row] = probs[row][labels[row]]\n",
    "nll_loss = -nll_loss.mean()\n",
    "\n",
    "print(nll_loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68b355cc2da9a16373f175e969c590b929502fd45fdce3f5fc38f9d16ba5406c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
