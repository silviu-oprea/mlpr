{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Greedy decoding: at each step, selecting the word with the highest probability.\n",
    "2. Beam search: Hold on to, say, the top two words (say, \"I\" and \"a\" for example). In the next step, run the model twice: once assuming the first output position was the word \"I\", and another time assuming the first output position was the word \"a\", and whichever version produced less error considering both positions #1 and #2 is kept. Repeat this for positions #2 and #3, and so on. This is called \"beam search\", where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we'll return two translations). These are both hyperparameters that you can experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature parameter: the skewness of the pronbability mass function determined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
