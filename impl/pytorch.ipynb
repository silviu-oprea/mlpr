{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "$\\def\\underline v{\\underline}$\n",
    "$\\def\\underline w{\\underline w}$\n",
    "$\\def\\underline x{\\underline x}$\n",
    "$\\def\\f{\\underline f}$\n",
    "$\\def\\y{\\underline y}$\n",
    "$\\def\\b{\\underline b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 2., 2., 2.],\n",
      "         [4., 4., 4., 4.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2., 2.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# (batch_size, doc_len)\n",
    "input = torch.tensor([\n",
    "    [2, 4, 3, 1],\n",
    "    [2, 0, 1, 1]\n",
    "])\n",
    "emb_fn = nn.Embedding(10, 3, padding_idx=1)\n",
    "emb_fn = nn.Embedding.from_pretrained(\n",
    "    torch.tensor([[0, 0, 0, 0],\n",
    "                  [1, 1, 1, 1],\n",
    "                  [2, 2, 2, 2],\n",
    "                  [3, 3, 3, 3],\n",
    "                  [4, 4, 4, 4]]).float()\n",
    ")\n",
    "# (batch_size, doc_len, emb_dim)\n",
    "emb = emb_fn(input)\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear\n",
    "\n",
    "Say we have a linear layer with 2 inputs and 3 outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first output is computed as $h_0 = x_0 w_{00} + x_1 w_{01} = \\underline x \\underline w_0^T$.\n",
    "\n",
    "Let us stack all weights of the layer in a matrix\n",
    "$$\n",
    "W = \\begin{bmatrix} \n",
    "  - \\underline w_0 - \\\\ \n",
    "  - \\underline w_1 - \\\\ \n",
    "  - \\underline w_2 - \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "of dimension (output dim, input dim). Each row $i$ has the weights of neuron $i$ that outputs $h_i$. Then $\\underline v h = \\underline x W^T$. Here, following PyTorch convention, both $\\underline v h$ and $\\underline x$ are column vectors. For batched inputs, $H = X W^T$.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "  x^{(0)}_0 & x^{(0)}_1 \\\\ \n",
    "  x^{(1)}_0 & x^{(1)}_1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "  w_{00} & w_{10} & w_{20} \\\\\n",
    "  w_{01} & w_{11} & w_{21}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "  h^{(0)}_0 & h^{(0)}_1 & h^{(0)}_2 \\\\ \n",
    "  h^{(1)}_0 & h^{(1)}_1 & h^{(1)}_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/impl-rnn-linear.drawio.svg\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 2.],\n",
       "         [1., 3., 3.],\n",
       "         [1., 4., 4.]],\n",
       "\n",
       "        [[1., 2., 2.],\n",
       "         [1., 3., 3.],\n",
       "         [1., 4., 4.]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 2\n",
    "hid_dim = 3\n",
    "\n",
    "# (batch_size  =2, doc_len=3, emb_dim=2)\n",
    "input = torch.tensor(\n",
    "    [[[1, 1],\n",
    "      [2, 2],\n",
    "      [3, 3]],\n",
    "      \n",
    "     [[1, 1],\n",
    "      [2, 2],\n",
    "      [3, 3]]],\n",
    ").float()\n",
    "\n",
    "linear_fn = nn.Linear(emb_dim, hid_dim)\n",
    "# This should have shape (out_features, in_features)\n",
    "linear_fn.weight = nn.Parameter(torch.tensor([[0, 0],\n",
    "                                              [0, 1],\n",
    "                                              [1, 0]]).float())\n",
    "# This should have shape (out_features,)\n",
    "linear_fn.bias = nn.Parameter(torch.tensor([1, 1, 1]).float())\n",
    "\n",
    "# (batch_size, doc_len, hid_dim)\n",
    "linear_fn(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Here is an illustration of the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/impl-rnn-rnn.drawio.svg\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a higher level illustration (type 2 on the right below).\n",
    "\n",
    "<img src=\"img/impl-rnn-rnn-2.jpg\" width=\"650\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11., 11.],\n",
      "         [12., 12.]],\n",
      "\n",
      "        [[21., 21.],\n",
      "         [22., 22.]],\n",
      "\n",
      "        [[31., 31.],\n",
      "         [32., 32.]]])\n",
      "torch.Size([3, 2, 5])\n",
      "tensor([[[-0.9819, -0.5371,  1.0000,  0.9950,  1.0000],\n",
      "         [-0.9871, -0.7188,  1.0000,  0.9984,  1.0000]],\n",
      "\n",
      "        [[-0.9998, -0.9091,  1.0000,  1.0000,  1.0000],\n",
      "         [-0.9998, -0.9500,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[-1.0000, -0.9851,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.9919,  1.0000,  1.0000,  1.0000]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 3, 5])\n",
      "tensor([[[-0.9871, -0.7188,  1.0000,  0.9984,  1.0000],\n",
      "         [-0.9998, -0.9500,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.9919,  1.0000,  1.0000,  1.0000]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# (batch size, doc len, emb dim)\n",
    "input = torch.tensor(\n",
    "    [\n",
    "        [[11, 11],\n",
    "         [12, 12]],\n",
    "      \n",
    "        [[21, 21],\n",
    "         [22, 22]],\n",
    "\n",
    "        [[31, 31],\n",
    "         [32, 32]]\n",
    "     ],\n",
    ").float()\n",
    "print(input)\n",
    "\n",
    "rnn_fn = nn.RNN(input_size=2, hidden_size=5, num_layers=1,\n",
    "                batch_first=True)\n",
    "\n",
    "out_across_time, hid_last_time = rnn_fn(input)\n",
    "\n",
    "# (batch size, doc len, emb dim * num directions)\n",
    "# (batch size, num layers * num directions, emb dim)\n",
    "\n",
    "print(out_across_time.size())\n",
    "print(out_across_time)\n",
    "\n",
    "print(hid_last_time.size())\n",
    "print(hid_last_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[11., 11.],\n",
      "        [21., 21.],\n",
      "        [31., 31.],\n",
      "        [41., 41.],\n",
      "        [12., 12.],\n",
      "        [22., 22.]]), batch_sizes=tensor([4, 2]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[[-0.1654, -0.0464, -0.0121, -0.6202, -0.2044,  0.3503],\n",
      "         [-0.1004, -0.0021,  0.0830, -0.3628, -0.0523,  0.4666]],\n",
      "\n",
      "        [[-0.2162, -0.0467, -0.0456, -0.6263, -0.2797,  0.3420],\n",
      "         [-0.1428,  0.0275,  0.0870, -0.3663, -0.1060,  0.4790]],\n",
      "\n",
      "        [[-0.1998, -0.0466, -0.0348, -0.3691, -0.1528,  0.4897],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[-0.2321, -0.0467, -0.0563, -0.3720, -0.2018,  0.5008],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# (batch size, doc len, emb dim)\n",
    "input = torch.tensor(\n",
    "    [\n",
    "        [[11, 11],\n",
    "         [12, 12]],\n",
    "\n",
    "        [[21, 21],\n",
    "         [22, 22]],\n",
    "\n",
    "        [[31, 31],\n",
    "         [1, 1]], # padding\n",
    "\n",
    "        [[41, 41],\n",
    "         [1, 1]] # padding\n",
    "     ]\n",
    ").float()\n",
    "\n",
    "# same as input.transpose(0, 1)\n",
    "input = torch.tensor( \n",
    "    [   # time step 1, i.e. embedding of first word for all documents in the batch\n",
    "        [[11., 11.],\n",
    "         [21., 21.],\n",
    "         [31., 31.],\n",
    "         [41., 41.]],\n",
    "\n",
    "        # time step 2\n",
    "        [[12., 12.],\n",
    "         [22., 22.],\n",
    "         [ 1.,  1.], # padding\n",
    "         [ 1.,  1.]] # padding\n",
    "    ]\n",
    ")\n",
    "# (batch size,)\n",
    "dl = torch.tensor([2, 2, 1, 1])\n",
    "\n",
    "# Sequences should be sorted in decreasing order\n",
    "packed = nn.utils.rnn.pack_padded_sequence(input, dl)\n",
    "\n",
    "# packed.data = torch.tensor(\n",
    "#     [\n",
    "#         # time step 1 across all documents\n",
    "#         [11., 11.],\n",
    "#         [21., 21.],\n",
    "#         [31., 31.],\n",
    "#         [41., 41.],\n",
    "#         # Time step 2 across all documents that have such a time step\n",
    "#         # Recall input is sorted buy length\n",
    "#         [12., 12.],\n",
    "#         [22., 22.]\n",
    "#     ]\n",
    "# ), of dim (sum of all doc lengths, emb dim).\n",
    "#\n",
    "# packed.batch_sizes = [4, 2].\n",
    "# Position t stores the number of documents in the batch that still have words\n",
    "# at time t.\n",
    "print(packed)\n",
    "\n",
    "rnn_fn = nn.RNN(input_size=2, hidden_size=3, num_layers=2, bidirectional=True)\n",
    "# out_across_time.data: (sum of lens of all batch docs, emb dim * num directions)\n",
    "# hid_last_time: (num layers * num directions, batch size, emb dim)\n",
    "out_across_time, hid_last_time = rnn_fn(packed)\n",
    "\n",
    "# (doc len, batch size, emb dim * num directions)\n",
    "out_across_time, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "    out_across_time,\n",
    "    padding_value=1.0\n",
    ")\n",
    "\n",
    "print(out_across_time.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence\n",
    "<img src=\"img/impl-rnn-s2s.png\" width=\"550\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
