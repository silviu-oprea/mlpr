{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ4haWRucYDg"
      },
      "source": [
        "# Download and tokenize the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LM2rk47kYkzS",
        "outputId": "21b00579-bb88-4d86-a704-911907a33979"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = \"./data\"\n",
        "\n",
        "with open(os.path.join(DATA_PATH, \"tiny_shakespeare.txt\"), \"r\", encoding=\"utf-8\") as fp:\n",
        "    corpus = fp.read()\n",
        "\n",
        "corpus[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bip3VrrlYx0-",
        "outputId": "6fb62d60-45cd-457e-93ab-0516143311ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
            "hello world!\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(list(set(corpus)))\n",
        "c_to_i = {c: i for i, c in enumerate(vocab)}\n",
        "i_to_c = {i: c for i, c in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def encode(txt):\n",
        "    return [c_to_i[c] for c in txt]\n",
        "\n",
        "def decode(i_lst):\n",
        "    return ''.join([i_to_c[i] for i in i_lst])\n",
        "\n",
        "txt = \"hello world!\"\n",
        "print(encode(txt))\n",
        "print(decode(encode(txt)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxAcqiBwZHiw",
        "outputId": "8ede68b3-8453-4cac-c567-ea98f564d3c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(corpus), dtype=torch.long)\n",
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5ff0I6VgZXrr"
      },
      "outputs": [],
      "source": [
        "num_train_ex = int(0.9 * len(data))\n",
        "train_data = data[:num_train_ex]\n",
        "val_data = data[num_train_ex:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnIXpIPLZahd"
      },
      "source": [
        "Block size, also known as context length, is the maximum input length.\n",
        "In a block size of 8, we have 8 (input, output) training examples, as shown below.\n",
        "\n",
        "This way, the transformer will have seen examples with input length ranging from 1 to `block_size`. This is useful because when we sample during inference (i.e. generate), we want the transformer to be able to complete sequences as short as 1 token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFrTV0v9Z0wi",
        "outputId": "3fceffea-6e1d-4c5b-896d-3fe3dc226b9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-a1T1ZmZ3wH",
        "outputId": "0fec7fc0-8c7e-4f56-debd-e0ae9c36fe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: tensor([18]) outputs: tensor(47)\n",
            "input: tensor([18, 47]) outputs: tensor(56)\n",
            "input: tensor([18, 47, 56]) outputs: tensor(57)\n",
            "input: tensor([18, 47, 56, 57]) outputs: tensor(58)\n",
            "input: tensor([18, 47, 56, 57, 58]) outputs: tensor(1)\n",
            "input: tensor([18, 47, 56, 57, 58,  1]) outputs: tensor(15)\n",
            "input: tensor([18, 47, 56, 57, 58,  1, 15]) outputs: tensor(47)\n",
            "input: tensor([18, 47, 56, 57, 58,  1, 15, 47]) outputs: tensor(58)\n"
          ]
        }
      ],
      "source": [
        "for t in range(block_size):\n",
        "    print('input:', train_data[:t + 1], 'outputs:', train_data[t + 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0zhR2c3dVr5"
      },
      "source": [
        "Now let's batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQnf8t5IdXfP",
        "outputId": "724ea39e-5eb2-4091-b9b6-c8dc655abfae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[51, 59, 57, 58,  1, 39, 61, 39],\n",
              "         [63,  6,  1, 61, 46, 53,  5, 57],\n",
              "         [39,  1, 41, 46, 47, 50, 42, 10],\n",
              "         [53, 59, 10,  0, 37, 53, 59,  1]]),\n",
              " tensor([[59, 57, 58,  1, 39, 61, 39, 63],\n",
              "         [ 6,  1, 61, 46, 53,  5, 57,  1],\n",
              "         [ 1, 41, 46, 47, 50, 42, 10,  0],\n",
              "         [59, 10,  0, 37, 53, 59,  1, 57]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 4 # Number of independent sequences processed in parallel\n",
        "block_size = 8 # Maximum context length\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "def get_batch(data):\n",
        "    start_positions = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    input = torch.stack([data[i: i + block_size] for i in start_positions])\n",
        "    output = torch.stack([data[i + 1: i + 1 + block_size] for i in start_positions])\n",
        "    return input, output\n",
        "\n",
        "def get_train_batch(): return get_batch(train_data)\n",
        "def get_val_batch(): return get_batch(val_data)\n",
        "\n",
        "get_train_batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3J5X_-PhT98"
      },
      "source": [
        "In the tensors above, we have 32 training examples, 8 for each row. For the first row:\n",
        "* input=[51], i.e. inputs[0:1]; output=59, i.e. outputs[0][0];\n",
        "* input=[51, 59], i.e. inputs[0:2]; output=57, i.e. outputs[0][1];\n",
        "* ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI0QZ4prl_nr"
      },
      "source": [
        "# Bigram language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9BuE5L37iCbO"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.time_embed_layer = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of size `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary.\n",
        "            targets (`torch.LondTensor` of size `(batch_size, sequence_length)`):\n",
        "                Indices of target tokens in the vocabulary.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_length, embed_dim)\n",
        "        time_embeds = self.time_embed_layer(input_ids)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Since embed_dim = vocab_size, time_embeds can be thought of as logits.\n",
        "            # The task is: given the current word, predict the next word.\n",
        "\n",
        "            batch_size, seq_len, embed_dim = time_embeds.size()\n",
        "            time_embeds = time_embeds.view(batch_size * seq_len, embed_dim)\n",
        "            targets = targets.view(batch_size * seq_len)\n",
        "            loss = F.cross_entropy(time_embeds, targets)\n",
        "\n",
        "        return time_embeds, loss\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # (batch_size, seq_len, embed_dim)\n",
        "            logits, _ = self(input_ids)\n",
        "\n",
        "            # Keep the logits for the last token only.\n",
        "            # (batch_size, vocab_size == embed_dim)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # (batch_size, vocab_size)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "\n",
        "            # Sample from the distribution (one sample (an index) from eahc row)\n",
        "            # (batch_size, 1)\n",
        "            next_ids = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the sample to the running sequence\n",
        "            # (batch_size, seq_len + 1)\n",
        "            input_ids = torch.cat((input_ids, next_ids), dim=1)\n",
        "        return input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZteupYTFmXz1",
        "outputId": "a5105a54-6660-4f57-f69e-c500c02d0f6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[58, 46,  1, 57, 43, 43, 57,  1],\n",
              "         [43,  1, 53, 44,  1, 58, 46, 43],\n",
              "         [58,  1, 47, 57,  5, 58,  0, 63],\n",
              "         [ 1, 53, 44,  1, 44, 50, 39, 58]]),\n",
              " tensor([[46,  1, 57, 43, 43, 57,  1, 46],\n",
              "         [ 1, 53, 44,  1, 58, 46, 43,  1],\n",
              "         [ 1, 47, 57,  5, 58,  0, 63, 53],\n",
              "         [53, 44,  1, 44, 50, 39, 58, 58]]))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm = BigramLanguageModel(len(vocab))\n",
        "input_ids, targets = get_train_batch()\n",
        "input_ids, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnCng2PQqv8g",
        "outputId": "ef05cd06-1dc7-42a0-c92a-42433c97a2c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 65]), tensor(4.2638, grad_fn=<NllLossBackward0>))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs, loss = lm(input_ids, targets)\n",
        "outputs.size(), loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fVcjIR-xtsr"
      },
      "source": [
        "For a language modelling head that assigns equal probability to all words, i.e. maximum entropy, the probability of each word would be 1/65, so the loss would be $-\\ln(1/65) \\approx 4.17$ (selecting one of those 1/65, where the target one-hot vector has a 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCmdniIoyerz",
        "outputId": "8c01f3a9-fdff-49c2-ccf7-fd13450e4489"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
              "        0.0154, 0.0154])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([1/65] * 65, dtype=torch.float32), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L2fDmKlGzNl-",
        "outputId": "cdafed50-aae7-414c-984c-893e22ead2f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nQB:L$J:QxkURQxsa vZNbSvahVTKvJRWDDlBXQj zlR&3p  tKma'bxVsn;kjOFBjqaWQj fkTWZQ;YIMlcGPPPiia?sml.LZMEQ\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_input_ids = torch.zeros((1, 1), dtype=torch.long)\n",
        "gen_ids = lm.generate(gen_input_ids, max_new_tokens=100)\n",
        "decode(gen_ids[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "326RLUa6CQQ1"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3yqYPBKrCRu9"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(lm.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4b7x9G_Cc2d",
        "outputId": "a99adfb4-9077-4cde-dc4e-69bd1357e073"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 10000/10000 [00:03<00:00, 3026.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.763643264770508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "losses = []\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "for steps in tqdm.tqdm(range(10000), desc=\"Training\"):\n",
        "    input_ids, targets = get_train_batch()\n",
        "\n",
        "    logits, loss = lm(input_ids, targets)\n",
        "    optimizer.zero_grad(set_to_none=True) # zero all gradients from prev step\n",
        "    loss.backward() # compute gradients for all parameters\n",
        "    optimizer.step() # use those gradients to update parameters\n",
        "    losses.append(loss.item())\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "ejNl7CeXrtyN",
        "outputId": "2ad20abb-0d95-4f4e-bdad-c8978ec619ac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: plot losses, gray line on a dark background, smoothed over 10 steps, small figure size\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.plot(\n",
        "    [\n",
        "        sum(losses[i : i + 10]) / 10\n",
        "        for i in range(0, len(losses), 10)\n",
        "    ],\n",
        "    color=\"gray\",\n",
        ")\n",
        "plt.gca().set_facecolor(\"black\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_6i6sqFmD6LM",
        "outputId": "c0ec3ffb-d551-4482-859c-ffee72cdc98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAr:\\nLAngomyqwet gan ithenthwin? bx?\\nonenoth TZf neeqknd hau fomyou aropoun luder\\nBe heit noures; n f'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_input_ids = torch.zeros((1, 1), dtype=torch.long)\n",
        "gen_ids = lm.generate(gen_input_ids, max_new_tokens=100)\n",
        "decode(gen_ids[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05oER38JtKvK"
      },
      "source": [
        "# Self-attention\n",
        "\n",
        "Assume we are given a tensor as the one below, of size (batch size, doc_len, emb_dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UASUcpyAtNF3"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([\n",
        "    [[1, 3],\n",
        "     [2, 1],\n",
        "     [0, 1]],\n",
        "\n",
        "    [[0, 1],\n",
        "     [5, 4],\n",
        "     [0, 0]]\n",
        "]).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I5_GdVavA0w"
      },
      "source": [
        "We would like to produce another where the representation of word $t \\in \\{1, \\ldots, \\text{doc\\_len}\\}$ is the sum representations of words $1, \\ldots, t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxjz0XpsvUgP"
      },
      "outputs": [],
      "source": [
        "target = torch.tensor([\n",
        "    [[1, 3],\n",
        "     [3, 4],\n",
        "     [3, 5]],\n",
        "\n",
        "    [[0, 1],\n",
        "     [5, 5],\n",
        "     [5, 5]]\n",
        "]).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yFVMwoNvkgW"
      },
      "source": [
        "Here is the inefficient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRrzjERTvnO-",
        "outputId": "1ccfc1d2-5502-4323-af97-d1aa377fb514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_bow = torch.zeros_like(x)\n",
        "batch_size, doc_len, emb_dim = x.size()\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(doc_len):\n",
        "        x_bow[b, t] = x[b, :t + 1].sum(dim=0)\n",
        "x_bow.allclose(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhoPdnUzwz0q"
      },
      "source": [
        "And now for the efficient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID0L6Mdcw1w6",
        "outputId": "8eec883a-0d3d-4dbc-8e3b-52992ce88cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[[1., 3.],\n",
            "         [2., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        [[0., 1.],\n",
            "         [5., 4.],\n",
            "         [0., 0.]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 3.],\n",
              "         [3., 4.],\n",
              "         [3., 5.]],\n",
              "\n",
              "        [[0., 1.],\n",
              "         [5., 5.],\n",
              "         [5., 5.]]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = torch.tril(torch.ones(3,3))\n",
        "print(mask)\n",
        "print(x)\n",
        "mask @ x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Smr5AUx0HE"
      },
      "source": [
        "We are multiplying mask of size (3, 3) with x of size (2, 3, 2). Pytorch will know to multiply mask with each (3, 2) element in x.\n",
        "\n",
        "Now let's do averaging instead of summing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18XL5hQgyS8q",
        "outputId": "135dbce5-b095-419f-c030-ff03139f829a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 3.0000],\n",
              "         [1.5000, 2.0000],\n",
              "         [1.0000, 1.6667]],\n",
              "\n",
              "        [[0.0000, 1.0000],\n",
              "         [2.5000, 2.5000],\n",
              "         [1.6667, 1.6667]]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = torch.tril(torch.ones(3, 3))\n",
        "mask[mask == 0] = float(\"-inf\")\n",
        "mask = torch.softmax(mask, dim=1)\n",
        "print(mask)\n",
        "\n",
        "mask @ x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, when computing the representation of the current token, all previous tokens are given the same weight.\n",
        "This is not great. Consider \"I had a pizza, it was very tasty\". When representing \"it\", (the representation of) \"pizza\" should have a higher weight that that of \"had\", for instance.\n",
        "\n",
        "For a character-level model, the representation of a e.g. vowel should depend on specific previous consonants.\n",
        "\n",
        "We want such weights to be data dependent. This is the problem that self-attention solves.\n",
        "\n",
        "Self-attention:\n",
        "* Given a token, call it the current token, compute an affinity score between the current token and every token in the sequence, resulting in T affinity scores.\n",
        "* To accomplish this, every token emits three vectors: a query, a key, and a value:\n",
        "  * query of a token: intuitively expresses what that token is looking for when considring other tokens. \"It\" looking for a referrant.\n",
        "  * key: information content that other queries can match against.\n",
        "  * value: what the token communicates about itself to other tokens.\n",
        "* The affinity score between the current token and another token is the dot product between query(current token) and key(other token).\n",
        "* When re-representing current token, we compute affinity between the current token and all previous tokens, including current token. We then represent current token as the weighted average of the values of all these tokens, where the weights are the affinity scores.\n",
        "* Summary: key (what I contain), query (what I am looking for), value (what I will communicate about myself).\n",
        "\n",
        "Analogy:\n",
        "* Directed graph. Every node has some value vector. This expresses the information content it communicates to nodes that it points to.\n",
        "* Given a node, compute a weighted average of the nodes that point to this node. We want the weights to be data dependent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[1., 3.],\n",
              "          [2., 1.],\n",
              "          [0., 1.]],\n",
              " \n",
              "         [[0., 1.],\n",
              "          [5., 4.],\n",
              "          [0., 0.]]]),\n",
              " torch.Size([2, 3, 2]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size, doc_len, emb_dim = x.size()\n",
        "x, x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 16])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B, T, C = x.size()\n",
        "head_size = 16 # the dimension of the key, query, and value vectors\n",
        "\n",
        "key_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
        "query_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
        "value_layer = torch.nn.Linear(emb_dim, head_size, bias=False)\n",
        "\n",
        "query = query_layer(x) # (B, T, C)\n",
        "key = key_layer(x)     # (B, T, C)\n",
        "value = value_layer(x) # (B, T, C)\n",
        "\n",
        "# (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "wei = query @ key.transpose(2, 1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
        "wei = torch.softmax(wei, dim=-1)\n",
        "\n",
        "out = wei @ value\n",
        "\n",
        "out.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "* There is no notion of space. This is why we need positional encoding. Meaning of a token might differ with position. E.g. verb particles in German added to the end of the sentence Self-attention vs convolution: notion of space exists in convolution.\n",
        "* Above we are implementing a _decoder_ / _masked_ self-attention block; when representing the current token, we do NOT consider future tokens. By contrast, in _encoder_ block, this restriction is removed; implemented by removing the masking, i.e. removing `wei = wei.masked_fill(tril == 0, float(\"-inf\"))`. The decoder is appropriate for language modelling; encoder for e.g. classification tasks, such as sentiment analysis.\n",
        "* In self-attention, keys, querries, and values, are computed from the same source. In _cross-attention_, also referred to as _encoder-decoder attention_ when using an encoder-decoder architecture, the querries are computed from current source, but keys and values from another source, from which we would like to pull information; for instance, from the (output of the) encoder blocks, that represent some context we'd like to condition on.\n",
        "* _Scaled_ attention divides `wei` by `sqrt(head_size)`. Assume querries and keys are unit Gaussians, i.e. follow a Gaussian distribution with unit variance. If we do not divide, the variance of wei will be multiplied by `head_size`. If we divide, it stays unit variance. Stated differently, the values in `wei` will simply be larger. As a result, the softmax will sharper, getting further away from a uniform, sharping towards the max, converging to a one-hot vector.\n",
        "* _Multi-head_ attention: apply multiple attentions in parallel (using different query, key, and value matrices) and concatenate their results. Having different initialisations, they capture different interactions. Different local minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.8659)\n",
            "tensor(0.7215)\n",
            "tensor(8.2634)\n",
            "tensor(0.5165)\n"
          ]
        }
      ],
      "source": [
        "Q = torch.randn(B, T, head_size)\n",
        "K = torch.randn(B, T, head_size)\n",
        "wei_1 = Q @ K.transpose(1, 2)\n",
        "wei_2 = Q @ K.transpose(1, 2) / head_size**0.5\n",
        "\n",
        "print(Q.var())\n",
        "print(K.var())\n",
        "print(wei_1.var())\n",
        "print(wei_2.var())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
