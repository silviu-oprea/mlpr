{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "\n",
    "Maximum likelihood (ML):\n",
    "- is an algorithm;\n",
    "- for fitting, i.e. estimating the parameters $\\underline\\theta$ of;\n",
    "- a probability distribution/measure, i.e. a parametric function $f_{\\underline\\theta}$ from a predetermined family;\n",
    "- given $N$ <u>presumed</u> samples from that distribution.\n",
    "\n",
    "Steps:\n",
    "1. Formulate the likelihood function $\\mathcal{L}(\\underline\\theta) = \\prod_{n=1}^N p(y^{(n)} | \\underline x^{(n)}; f_{\\underline\\theta})$. Treat this as a \"loss\" or \"cost\" function.\n",
    "2. Find parameters $\\underline \\theta$ that maximise $\\mathcal{L}(\\underline \\theta)$.\n",
    "\n",
    "We usually solve the equivalent problem of minimising $- \\log \\mathcal{L}(\\underline \\theta)$, easier for easier computation.\n",
    "\n",
    "Based on the distribution family chosen, we get different losses. Here are some common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square loss\n",
    "Assume\n",
    "$$\n",
    "p\\left(y^{(n)} | x^{(x)}; f_{\\underline \\theta}\\right) = \\mathcal{N} \\left( y^{(n)}; \\mu = f\\left( x^{(n)} \\right), \\sigma_y^2 \\right),\n",
    "$$\n",
    "for some fixed $\\sigma_y^2$, which (assume) can be estimated with e.g. a neural network $f_{\\underline\\theta}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\mathcal{L}(\\underline \\theta) = c_1 \\sum_n \\left( y^{(n)} - f_{\\underline \\theta}\\left( \\underline x^{(n)} \\right) \\right)^2 + c_2,\n",
    "$$\n",
    "for some (irrelevant for optimisation) constants $c_1$ and $c_2$.\n",
    "\n",
    "This is the regular sum of squares loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy loss\n",
    "\n",
    "Think about binary classification tasks. That is, assume\n",
    "$$\n",
    "p\\left( y^{(n)} | x^{(n)}; f_{\\underline \\theta} \\right) =\n",
    "\\begin{cases}\n",
    "    p^{(n)}, & \\text{if } y^{(n)} = 1\\\\\n",
    "    1 - p^{(n)}, & \\text{if } y^{(n)} = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "where $p^{(n)} \\stackrel{\\text{e.g.}}{=} \\sigma\\left( f_{\\underline\\theta}\\left( x^{(n)} \\right) \\right)$, which (assume) can be estimated with a e.g. neural network $f_{\\underline\\theta}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\mathcal{L}(\\underline\\theta) = \\sum_{n:y^{(n)} = 1} \\log p^{(n)} - \\sum_{n: y^{(n)} = 0} \\log \\left( 1 - p^{(n)} \\right) = \\sum_n \\left( p^{(n)} \\right)^{y^{(n)}} \\left( 1 - p^{(n)} \\right)^{\\left( 1 - y^{(n)} \\right)}.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68b355cc2da9a16373f175e969c590b929502fd45fdce3f5fc38f9d16ba5406c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
